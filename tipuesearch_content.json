{"pages":[{"text":"Performace analysis how to find the bottleneck of the server performace. Check out this fabulous pic from http://www.brendangregg.com/linuxperf.html and this great slices: http://www.slideshare.net/brendangregg/linux-performance-tools First you need to monitor you server and services, you can use munin, there are a lot of options. When the service slow down, I would login to the service, using the following tools: htop htop is great, it's like top , but better, I highly recommend this tool. watch the load , if load > number of CPUs, may mean CPU saturation vmstat $ vmstat 1 procs ----------- memory ---------- --- swap -- ... r b swpd free buff cache si so ... 9 0 0 29549320 29252 9299060 0 ... 2 0 0 29547876 29252 9299332 0 ... 4 0 0 29548124 29252 9299460 0 ... 5 0 0 29548840 29252 9299592 0 ... The procs data reports the number of processing jobs waiting to run and allows you to determine if there are processes \"blocking\" your system from running smoothly. The r column displays the total number of processes waiting for access to the processor. The b column displays the total number of processes in a \"sleep\" state. I usually see r value, if it's big, it means the server is in a high load. For other data meaning, ref: https://www.linode.com/docs/uptime/monitoring/use-vmstat-to-monitor-system-performance iostat iotop Block I/O status $ iostat - xmdz 1 1st output is since boot. For more information refer 24 iostat, vmstat and mpstat Examples for Linux Performance Monitoring iotop can show which process use most I/O. pidstat Very useful process stats. eg, by-thread, disk I/O: iftop iftop show the network usage with other hosts. Nginx tunning somaxconn fs-max","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/10/11/hight-throughput-web-server-tunning.html","title":"Hight Throughput Web Server Tunning"},{"text":"Introduction Gevent is an efficient coroutine-based asynchronous concurrency framework, it is based on libev and greenlet . If you don't know how greenlet works, recommend to read this . If you only want to know how to use gevent, recommend you read http://sdiehl.github.io/gevent-tutorial/ . Greenlet diagram First an example from official website. >>> import gevent >>> from gevent import socket >>> urls = [ ' www . google . com ' , ' www . example . com ' , ' www . python . org ' ] >>> jobs = [ gevent . spawn ( socket . gethostbyname , url ) for url in urls ] >>> gevent . joinall ( jobs , timeout = 2 ) >>> [ job . value for job in jobs ] [ ' 74.125.79.106 ' , ' 208.77.188.166 ' , ' 82.94.164.162 ' ] In this example gevent retrieve three ips from web concurrently. jobs = [gevent.spawn(socket.gethostbyname, url) for url in urls] create three task greenlets, and gevent.joinall(jobs, timeout=2) wait all the three task greenlets finished. This is the greenlet diagram of the program. +-----+ +----> | task | | +-----+ | +-------+ +-----++ +-----+ | main +----> | hub |-----> | task | +-------+ +-----++ +-----+ | | +-----+ +----> | task | +-----+ How it works main is the default greenlet, task is the greenlet that do the real job, hub greenlet is the core of gevent, it cooperates all other greenlets, including main . So what is hub ? hub is where gevent use libev . When gevent initializes, it create hub first, and hub is singleton, only one hub exists in the thread. def get_hub ( * args , ** kwargs ): \"\"\"Return the hub for the current thread. If hub does not exists in the current thread, the new one is created with call to :meth:`get_hub_class`. \"\"\" global _threadlocal try : return _threadlocal . hub except AttributeError : hubtype = get_hub_class () hub = _threadlocal . hub = hubtype ( * args , ** kwargs ) return hub When use do gevent.spawn , gevent will create a task greenlet using hub as parent. spawn = Greenlet . spawn class Greenlet ( greenlet ): \"\"\"A light-weight cooperatively-scheduled execution unit.\"\"\" def __init__ ( self , run = None , * args , ** kwargs ): hub = get_hub () greenlet . __init__ ( self , parent = hub ) def start ( self ): \"\"\"Schedule the greenlet to run in this loop iteration\"\"\" self . _start_event = self . parent . loop . run_callback ( self . switch ) @classmethod def spawn ( cls , * args , ** kwargs ): \"\"\"Return a new :class:`Greenlet` object, scheduled to start. The arguments are passed to :meth:`Greenlet.__init__`. \"\"\" g = cls ( * args , ** kwargs ) g . start () return g At the begining, we said gevent use libev , here self.parent.loop.run_callback(self.switch) , loop is from libev , we register the new created task greenlet to loop . when the loop runs, it will execute the task greenlet. So when loop runs, when main do some block operations, it will switch to hub , def run ( self ): assert self is getcurrent (), 'Do not call Hub.run() directly' while True : loop = self . loop loop . error_handler = self try : loop . run () finally : loop . error_handler = None # break the refcount cycle self . parent . throw ( LoopExit ( 'This operation would block forever' )) loop.run() will be executed, so loop can choose one greenlet to run, when the greenlet is also block or finished, it will choose another one to run. Next explain monkeypatch Ref http://blog.segmentfault.com/fantix/1190000000613814 https://github.com/surfly/gevent","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/09/23/gevent-internals.html","title":"Gevent Internals"},{"text":"今天在阅读 Django performance tips 时发现关闭keepalive可以提高效率，原文如下： Turn off KeepAlive I don't totally understand how KeepAlive works, but turning it off on our Django servers increased performance by something like 50%. Of course, don't do this if the same server is also serving media… but you're not doing that, right? HTTP keep-alive即是HTTP persistent connection，维基百科有详细说明：http://en.wikipedia.org/wiki/HTTP_persistent_connection 如果使用keep-alive功能，多个http请求会使用同一个tcp连接，这样可节省多次建立连接的时间及资源消耗， 下图显示了是否使用keep-alive功能的tcp连接对比图： http 1.0默认不开启，可进行指定，http 1.1默认开启。 上面我们看到启用keep-alive可节省多次建立连接的时间，但是服务器端要保持连接状态(nginx默认保持74s)， 这样会影响服务器端的性能。 而且目前网络带宽比较大，建立连接的延时比较少， 所以keep-alive在很多情况下会导致性能下降。这就验证了一开始那篇文章指出的在django servers中关闭keep-alive 功能可大约提升50%的性能。 所以建议关系keep-alive功能，除非有特殊需求。 下面介绍如何nginx中关系keep-alive功能 nginx文档 中有详细说明: keepalive_timeout Syntax: keepalive_timeout timeout [ header_timeout ] Default: 75s Context: http server location Reference: keepalive_timeout The first parameter assigns the timeout for keep-alive connections with the client. The server will close connections after this time. The optional second parameter assigns the time value in the header Keep-Alive: timeout=time of the response. This header can convince some browsers to close the connection, so that the server does not have to. Without this parameter, nginx does not send a Keep-Alive header (though this is not what makes a connection \"keep-alive\"). 如果我们要关闭keep-alive功能，可在http, server或location中设置 keepalive_timeout 0; 如： server { listen port ; server_name address ; access_log / var / log / nginx / mysite - access . log ; error_log / var / log / nginx / mysite - error . log ; keepalive_timeout 0 ; location / { ........ } } 下图给出了keepalive_timeout设置前后，http请求的header中connection的变化： 设置 keepalive_timeout 0; 之后： 笔者并没有对keep-alive做相应性能方面的benchmark，所以不确定keep-alive对性能的影响，而且keep-alive对性能的影响也与应用场景有关。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/09/23/http-keep-alive.html","title":"HTTP keep-alive"},{"text":"## insert current date \"map F3 to insert current date nnoremap <F3> \" = strftime ( \"%Y-%m-%d (%a)\" ) < CR > P inoremap < F3 > < C -R >= strftime ( \"%Y-%m-%d (%a)\" ) < CR > http://vim.wikia.com/wiki/Insert_current_date_or_time http://vim.wikia.com/wiki/Mapping_keys_in_Vim_-_Tutorial_(Part_1) http://stackoverflow.com/questions/1218390/what-is-your-most-productive-shortcut-with-vim/1220118#1220118","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/05/05/insert-datetime-in-vim.html","title":"Insert datetime in vim"},{"text":"Sometimes I need to prepare a presentation, there are a lot of tools I can use, like keynotes, powerpoint, latex. I used to use latex beamer, it can generate very beautiful slides, and there are a lot of themes to choos. But the latex source code is not easy to write and clear. I love markdown, which is very popular now, I want to write source code using markdown, and generate a pdf file as beautiful as beamer. There is a powerful tool can do this, it's pandoc( http://johnmacfarlane.net/pandoc/ ), it can convert one markup format to another, and support most of formats. Here I use pandoc to convert markdown to beamer pandoc - V theme : Warsaw - t beamer - o slides . pdf slides . md I specify theme Warsaw to use with -V option. pandoc official website show how to wite markdown to producing slides: http://johnmacfarlane.net/pandoc/demo/example9/producing-slide-shows-with-pandoc.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/27/markdown-to-slides.html","title":"Use markdown to create presentation"},{"text":"caution : need gevent(>= 1.0), and uwsgi(>= 1.4) https://github.com/SiteSupport/gevent/downloads uwsgi install https://uwsgi-docs.readthedocs.org/en/latest/Install.html https://uwsgi-docs.readthedocs.org/en/latest/Gevent.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/config-gevent-on-uwsgi.html","title":"config gevent on uwsgi"},{"text":"下面介绍python下的一个用于项目部署的工具 fabric ，要学习fabric推荐阅读其官方文档， 本文给只给出一个简单的介绍及一些基本的功能。 根据官方文档的介绍: Fabric is: A tool that lets you execute arbitrary Python functions via the command line; A library of subroutines (built on top of a lower-level library) to make executing shell commands over SSH easy and Pythonic. 首先安装fabric pip install fabric ，安装完fabric之后，系统会多出fab命令。 fab命令在执行时会读取当前目录下的fabfile.py文件，并根据fabric的参数来执行文件中的函数。 fabfile.py def hello () : print ( \"Hello world!\" ) hello为一个task，执行hello如下： $ fab hello Hello world ! Done . 我们看到hello并没有参数，fab也支持给task加参数。 fab可以用于项目部署，服务器管理。它可以通过ssh执行服务器上的shell命令，当然也可以执行本地的shell命令。 例如： local执行本地命令，run执行服务器端命令，sudo以root权限执行服务器端命令。 fab可以通过设置env.hosts变量来在所有host中执行某一task。 fab也提供了一些decorator，如hosts, roles来设置在某些host上执行task。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/fabric.html","title":"fabric"},{"text":"最近在调mongodb的性能，发现currentop中有很多长时间运行的查询，从currentop中可以看到查询来源ip:port。 那么可以到ip机器上查询port对应的进程号，进而找到发出此查询的应用。 根据port来查询进程号有多种方案： netstat， sudo netstat -anp | grep <port> 关于netstat的使用参考：http://www.thegeekstuff.com/2010/03/netstat-command-examples/ fuser，找到使用tcp 7000端口号的进程： sudo fuser 7000/tcp lsof lsof - i : portNumber lsof - i tcp : portNumber lsof - i udp : portNumber lsof - i : 80 通过这几种方式找到进程号后，可以利用 ps -eaf | grep <pid> 找到此进程的执行命令， 并可以通过 /proc/<pid>/cwd 找到此进程的执行目录。 参考：http://www.cyberciti.biz/faq/what-process-has-open-linux-port/","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/find-out-which-process-open-a-linux-port.html","title":"Find out which process open a linux port"},{"text":"网络上经常有文章说mongo会需要大量内存，那么mongo到低需要多少内存。 MongoDB实际需要的内存大小取决于working set的大小，working set是mongo完成操作所需要的所有文档及索引。对于一个collection而言，如果不需要访问每一条记录，不需要所有的记录都在内存中。working set最好都在内存中，以保证好的性能，如果不都在内存中会出现比较多的page fault。 查看working set大小可用如下命令： db.runCommand( { serverStatus: 1, workingSet: 1 } ) serverStatus.mem.resident The value of resident is roughly equivalent to the amount of RAM, in megabytes (MB), currently used by the database process. In normal use this value tends to grow. In dedicated database servers this number tends to approach the total amount of system memory. 所以如果resident没有超出总的内存大小，此时内存是足够的，当然也要看一下page faults是不是很频繁。 Does MongoDB require a lot of RAM? Not necessarily. It's certainly possible to run MongoDB on a machine with a small amount of free RAM. MongoDB automatically uses all free memory on the machine as its cache. System resource monitors show that MongoDB uses a lot of memory, but its usage is dynamic. If another process suddenly needs half the server's RAM, MongoDB will yield cached memory to the other process. Technically, the operating system's virtual memory subsystem manages MongoDB's memory. This means that MongoDB will use as much free memory as it can, swapping to disk as needed. Deployments with enough memory to fit the application's working data set in RAM will achieve the best performance.","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/how-mongodb-use-memory.html","title":"How mongodb use memory"},{"text":"今天读了A. Jesse Jiryu Davis的一篇 文章 ，关于计算机专业的学生如何让自己突出，找到 自己喜欢的工作。 开篇作者就指出要想办法让自己突出，不要与其他学生都一样，如果你是一个黑客，那你就要表现出来。 仅仅学校课堂学的东西是不够的 要有气质、魅力。 好像计算机专业的学生不需要，错了！程序员也要和人交流，打交道，尤其是团队之间，如果你有魅力，会更容易和人交流，沟通效率更高。很多程序员也许都知道沟通的重要性，嘴上确说不需要，其实是在给自己找借口。 独立的项目，不一样的语言，不一样的课程 如果有创意想法，就实现它，这将会给你增加很多的分数， 学一门其它的编程语言，不要只局限于课堂上教的。 从长远来看，最好参与一些开源项目，一开始可能很难找到开源项目的一些问题，即使找到了，可能也写不出高质量的patch，可以先自己建立一个项目，自己开发。 当然也有例外：你可以修改一些python的库，使其兼容python 3；或者将一种语言上热门的库移植到另一种语言上(在另一种语言上重新实现)，如果这个语言上还没有相应的库。 对于我们在职的程序员，这些建议仍然适用，我们一定要有自己可拿出的东西来，有一些自己的项目，能与开源。 非常重要的一点，一定要有气质、魅力，善于和另人交流，杜绝屌丝。还有程序员要注意身体哦。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/job.html","title":"Job"},{"text":"Introduction Logbook是用来取代标准库logging的一个log系统，根据官方的介绍它让记log变的有趣， logbook目前还处于开发阶段。可以参考官方 core feature logbook与logging差别比较大，logbook和logging基本都是由：logger, handler, filter等组件组成，但是在logbook中这些组件之间的关系发生的根本的变化。并且logbook提供 了更多的组件，如：processor。 New Feature 在logbook中一个非常大的变化是大量使用 stack , 一开始采用logging的方式去理解logbook，发现很不适应，真正去了解之后，发现这种方式有非常强的灵活性， 可以非常灵活的组织handler，而不用去处理handler与logger的有关系，将logger与handler解耦合。 目前logbook中存在三种stack，分别用来存储Handler, Processor, Flag。 每一句log都会经过这三个stack中的每一个对象处理。假如目前存储handler的stack中目前 有两个handler h1, h2: | h1 | | ____ | | h2 | | ____ | 那每一句log都会被h1处理完后被h2再处理。 Handler, Processor, Flag必须加到stack中才能启到作用，可以使用 push_application() 及 pop_application() 。 logbook中也提供了 with 来完成对stack的push和pop。 logbook增加了很多的handler，甚至都有将log发到twitter上的handler。 logbook有非常多的特性，相信以后也会增加更多有趣的特性，建议阅读其官方文档：https://logbook.readthedocs.org/en/latest/ Examples 下面给出一些简单的使用示例，有些是官方文档上给出的。 开始使用logbook可以不用做任何配置： from logbook import warn warn ( 'This is too cool for stdlib' ) [ 2013 - 05 - 18 14 : 29 ] WARNING : Generic : This is too cool for stdlib 上面的代码会将log记录到stderr中。是不是非常简单，都不需要设置handler，logbook有非常好的默认配置， 在很多情况下你不需要改动即可直接使用。 使用handler： from logbook import FileHandler , info file_handler = FileHandler ( 'logbook.log' , level = 'INFO' ) with file_handler : info ( 'This is logged in a file' ) TestHandler logbook提供了TestHandler，可以用来测试log，使用非常方便，参考文档： https://logbook.readthedocs.org/en/latest/api/handlers.html#logbook.TestHandler MailHandler https://logbook.readthedocs.org/en/latest/api/handlers.html#logbook.MailHandler TODO ThreadedWrapperHandler https://logbook.readthedocs.org/en/latest/api/queues.html#logbook.queues.ThreadedWrapperHandler TODO FingersCrossedHandler logbook提供了一个特殊的\"fingers crossed\" handler，这个handler作为一个wrapper，是用来封装其它 handler的，这个handler有一个特殊的功能，它会将所有的log记录到内存中，当某一些log的级别 (debug, info, warning, error)超过FingersCrossedHandler设置的级别时，所有在内存中的log以及后面的log 都将被记录到这个handler中。当log级别没有到达时，所有的log都不会被FingersCrossedHandler处理。 这一handler很适合用在web application中，当某一错误产生时记录相关的request。 FingersCrossedHandler默认设置的级别为ERROR 下面给出两个示例代码： from logbook import info , error , FingersCrossedHandler , FileHandler file_handler = FileHandler ( 'bar.log' ) handler = FingersCrossedHandler ( file_handler ) with handler : info ( 'hello info' ) error ( 'hello error' ) 会在bar.log文件中输出两句log from logbook import info , error , FingersCrossedHandler , FileHandler file_handler = FileHandler ( 'bar.log' ) handler = FingersCrossedHandler ( file_handler ) with handler : info ( 'hello info' ) info ( 'hello info2' ) 不会有任何log输出 使用Processors import os def inject_cwd ( record ): record . extra [ 'cwd' ] = os . getcwd () with Processor ( inject_cwd ): # all logging calls inside this block in this thread will now # have the current working directory information attached. ... 注意 ： Processor只有当log被某一个handler处理的时候才会执行，否则processor永远不会招待。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/logbook.html","title":"Logbook"},{"text":"Mac sshfs Mac 挂载远程目录 http://osxfuse.github.com/ install OXSFUSE and SSHFS TF-IDF http://www.ruanyifeng.com/blog/2013/03/tf-idf.html http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html http://www.ruanyifeng.com/blog/2013/03/automatic_summarization.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/misc.html","title":"misc"},{"text":"query after update slow 这两天有一个接口的速度非常慢，主要是执行一条查询语句非常慢，有时耗时达200秒，于是找到了这条查询语句单独在mongo shell中执行，在mongo shell中执行并不慢，慢慢调试后最终找到问题，原因是在查询语句执行之前执行了两次更新语句。 利用pymongo执行更新时，如果不设置safe=True，mongo会及时返回函数调用，但更新并没有真正执行完，mongo进程在后台继续执行，所以当接着执行查询时，此时更新没有结束，数据库会处于lock状态，查询等待数据库解锁，所以查询非常慢。 把更新语句去除后，查询恢复正常。 Concurrency MongoDB global lock to ga Tips If Write Heavy Global Lock is Global As you probably know, MongoDB has a global lock. The longer your writes take, the higher your lock percentage is. Updating documents that are in RAM is super fast. Updating documents that have been pushed to disk, first have to be read from disk, stored in memory, updated, then written back to disk. This operation is slow and happens while inside the lock. Updating a lot of random documents that rarely get updated and have been pushed out of RAM can lead to slow writes and a high lock percentage. More Reads Make For Faster Writes The trick to lowering your lock percentage and thus having faster updates is to query the document you are going to update, before you perform the update. Querying before doing an upsert might seem counter intuitive at first glance, but it makes sense when you think about it. The read ensures that whatever document you are going to update is in RAM. This means the update, which will happen immediately after the read, always updates the document in RAM, which is super fast. I think of it as warming the database for the document you are about to update. http://www.mongotips.com/b/lower-lock-and-number-of-slow-queries/","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/mongo.html","title":"Mongo"},{"text":"最近mongo的性能","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/mongo-performance.html","title":"Mongo Performance"},{"text":"最近升级了mongodb，用mongo连接mongod后出现如下warning: Server has startup warnings : Mon Oct 29 14 : 45 : 23 [ initandlisten ] Mon Oct 29 14 : 45 : 23 [ initandlisten ] ** WARNING : You are running on a NUMA machine . Mon Oct 29 14 : 45 : 23 [ initandlisten ] ** We suggest launching mongod like this to avoid performance problems : Mon Oct 29 14 : 45 : 23 [ initandlisten ] ** numactl -- interleave = all mongod [ other options ] 随即查了下 NUMA 是什么，有多篇文章均提到了NUMA会导致mongodb的性能问题，而且官方文档也有说明。 To disable NUMA for MongoDB, use the numactl command and start mongod in the following manner: numactl -- interleave = all / usr / bin / local / mongod Adjust the proc settings using the following command: echo 0 > / proc / sys / vm / zone_reclaim_mode To fully disable NUMA you must perform both operations. However, you can change zone_reclaim_mode without restarting mongod. For more information, see documentation on Proc/sys/vm. 在此做一个整理： NUMA：NUMA是多核心CPU架构中的一种，其全称为Non-Uniform Memory Access，简单来说就是在多核心CPU中，机器的物理内存是分配给各个核的，架构简图如下所示： 每个核访问分配给自己的内存会比访问分配给其它核的内存要快，有下面几种访问控制策略： 缺省(default)：总是在本地节点分配（分配在当前进程运行的节点上）； 绑定(bind)：强制分配到指定节点上； 交叉(interleave)：在所有节点或者指定的节点上交织分配； 优先(preferred)：在指定节点上分配，失败则在其他节点上分配。 上面文章中最后使用numactl –interleave命令就是指定其为交叉共享模式。 我们知道虚拟内存机制是通过一个中断信号来通知虚拟内存系统进行内存swap的，所以这个irqbalance进程忙，是一个危险信号，在这里是由于在进行频繁的内存交换。这种频繁交换现象称为swap insanity，在MySQL中经常提到，也就是在NUMA框架中，采用不合适的策略，导致核心只能从指定内存块节点上分配内存，即使总内存还有富余，也会由于当前节点内存不足时产生大量的swap操作。 Install 在Ubuntu下以numactl的命令启动mongodb，要先安装numactl sudo apt - get install numactl 参考： http://docs.mongodb.org/manual/administration/production-notes/#production-numa http://huoding.com/2011/08/09/104 http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/ http://blog.nosqlfan.com/html/2772.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/mongodb-numa-dao-zhi-de-xing-neng-wen-ti.html","title":"Mongodb NUMA 导致的性能问题"},{"text":"","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/mongodb-replication-set.html","title":"MongoDB Replication Set"},{"text":"Writing plugins global attribute: http://munin-monitoring.org/wiki/protocol-config redis munin 有改动，总是说Redis找不到，一会儿一下问题，做一个patch mulitgraph: http://munin-monitoring.org/wiki/protocol-multigraph fw_conntract fw_forwarded_local timeout 配置好munin之后，fw_conntrack总是报警，读不到数据，主要原因是执行fw_conntrack超时，munin plugin的默认超时时间是10s，超时主要是因为 cat /proc/net/ip_conntrack ，对于网络请求少的服务器，此语句会很快执行完，但对于网络请求比较多的服务器，此语句耗时可能要超过30s。 Google的相应问题，此问题早已被人解决，解决方案见如下链接： http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=565565 主要就是用 conntrack -L 代替 cat /proc/net/ip_conntrack ，要先安装 sudo apt-get install conntrack limits limit 用于检查每个plugin的field的值是否超出设定的warning和critical，并做出相应的警报。 warning和critical是运行插件config输出的每个filed的warning和critical来设置的。 有时我们在 /etc/munin/plugin-conf.d/ 中会看到 env.warning value ，这是因为插件会读取 env.warning 的值，并根据此值设置每个field的warning。如果插件不读取 env.warning 的值，即使设置也不会启作用，munin不会自动设置。 关于warning, critical的设置可参考: {fieldname}.warning , {fieldname}.critical} http://munin-monitoring.org/wiki/protocol-config http://munin-monitoring.org/wiki/munin-man#munin-html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/munin.html","title":"munin"},{"text":"thread cache size http://www.dbasquare.com/kb/mysql-and-thread-cache-size/ query_cache_type可以设置为0(OFF)，1(ON)或者2(DEMOND)，分别表示完全不使用query cache，除显式要求不使用query cache（使用sql_no_cache）之外的所有的select都使用query cache，只有显示要求才使用query cache（使用sql_cache) show global status like ''; show global variables like ''; innodb_file_per_table mysql io 延时计算 mysql backup 相较前几种方法，备份数据文件最为直接、快速、方便，缺点是基本上不能实现增量备份。为了保证数据的一致性，需要在靠背文件前，执行以下 SQL 语句： FLUSH TABLES WITH READ LOCK; 也就是把内存中的数据都刷新到磁盘中，同时锁定数据表，以保证拷贝过程中不会有新的数据写入。这种方法备份出来的数据恢复也很简单，直接拷贝回原来的数据库目录下即可。 注意，对于 Innodb 类型表来说，还需要备份其日志文件，即 ib_logfile* 文件。因为当 Innodb 表损坏时，就可以依靠这些日志文件来恢复。 Innodb 表则可以通过执行以下语句来整理碎片，提高索引速度： ALTER TABLE tbl_name ENGINE = Innodb; 这其实是一个 NULL 操作，表面上看什么也不做，实际上重新整理碎片了。 为了不影响线上业务，实现在线备份，并且能增量备份，最好的办法就是采用主从复制机制(replication)，在 slave 机器上做备份。 mysql replication switch master and slave one slave and one master Master: set read lock FLUSH TABLES WITH READ LOCK; Slave: show processlist; when you see 'Has read all relay log', it means the slave is updated with master. stop slave; reset master; chane the code to operate on slave; Master: UNLOCK TABLES; make the old master to be a slave; change master to MASTER_HOST = '10.18.10.21', MASTER_USER = 'sns', MASTER_PASSWORD = '123'; [mysqld] log-bin=mysql-bin server-id=1 After making the changes, restart the server. Note If you omit server-id (or set it explicitly to its default value of 0), a master refuses connections from all slaves. Note For the greatest possible durability and consistency in a replication setup using InnoDB with transactions, you should use innodb_flush_log_at_trx_commit=1 and sync_binlog=1 in the master my.cnf file. Note Ensure that the skip-networking option is not enabled on your replication master. If networking has been disabled, your slave will not able to communicate with the master and replication will fail. 2. After connecting to the server as root, you can add new accounts. The following statements use GRANT to set up four new accounts: mysql> CREATE USER 'monty'@'localhost' IDENTIFIED BY 'some_pass'; mysql> GRANT ALL PRIVILEGES ON . TO 'monty'@'localhost' -> WITH GRANT OPTION; mysql> CREATE USER 'monty'@'%' IDENTIFIED BY 'some_pass'; mysql> GRANT ALL PRIVILEGES ON . TO 'monty'@'%' -> WITH GRANT OPTION; mysql> CREATE USER 'admin'@'localhost'; mysql> GRANT RELOAD,PROCESS ON . TO 'admin'@'localhost'; mysql> CREATE USER 'dummy'@'localhost'; The accounts created by these statements have the following properties: Two of the accounts have a user name of monty and a password of some_pass. Both accounts are superuser accounts with full privileges to do anything. The 'monty'@'localhost' account can be used only when connecting from the local host. The 'monty'@'%' account uses the '%' wildcard for the host part, so it can be used to connect from any host. It is necessary to have both accounts for monty to be able to connect from anywhere as monty. Without the localhost account, the anonymous-user account for localhost that is created by mysql_install_db would take precedence when monty connects from the local host. As a result, monty would be treated as an anonymous user. The reason for this is that the anonymous-user account has a more specific Host column value than the 'monty'@'%' account and thus comes earlier in the user table sort order. (user table sorting is discussed in Section 6.2.4, \"Access Control, Stage 1: Connection Verification\".) The 'admin'@'localhost' account has no password. This account can be used only by admin to connect from the local host. It is granted the RELOAD and PROCESS administrative privileges. These privileges enable the admin user to execute the mysqladmin reload, mysqladmin refresh, and mysqladmin flush-xxx commands, as well as mysqladmin processlist . No privileges are granted for accessing any databases. You could add such privileges later by issuing other GRANT statements. The 'dummy'@'localhost' account has no password. This account can be used only to connect from the local host. No privileges are granted. It is assumed that you will grant specific privileges to the account later. The statements that create accounts with no password will fail if the NO_AUTO_CREATE_USER SQL mode is enabled. To deal with this, use an IDENTIFIED BY clause that specifies a nonempty password. To check the privileges for an account, use SHOW GRANTS: mysql> SHOW GRANTS FOR 'admin'@'localhost'; +-----------------------------------------------------+ | Grants for admin@localhost | +-----------------------------------------------------+ | GRANT RELOAD, PROCESS ON . TO 'admin'@'localhost' | +-----------------------------------------------------+ 16.1.1.5. Creating a Data Snapshot Using mysqldump : http://dev.mysql.com/doc/refman/5.1/en/replication-howto-mysqldump.html Obtaining the Replication Master Binary Log Coordinates : http://dev.mysql.com/doc/refman/5.1/en/replication-howto-masterstatus.html If a slave uses the default host-based relay log file names, changing a slave's host name after replication has been set up can cause replication to fail with the errors Failed to open the relay log and Could not find target log during relay log initialization. This is a known issue (see Bug #2122). If you anticipate that a slave's host name might change in the future (for example, if networking is set up on the slave such that its host name can be modified using DHCP), you can avoid this issue entirely by using the --relay-log and --relay-log-index options to specify relay log file names explicitly when you initially set up the slave. This will make the names independent of server host name changes.","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/mysql-xing-neng-diao-you.html","title":"Mysql 性能调优"},{"text":"nginx backlog Root Directory nginx 默认根目录， 如果在配置文件中指定了root，则根目录即为root指定的目录 如果通过编译安装的nginx，则根目录为编译时指定的prefix目录 如果通过包管理器安装，则根目录与发行版相关。例如ubuntu版本的根目录为： /usr/share/nginx/www Custome error page nginx 有个默认的错误显示页面 Ref http://wiki.nginx.org/Pitfalls","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/nginx-tips.html","title":"nginx tips"},{"text":"介绍 peclian 是一个python版本的表态页面生成器，类似于jekyll","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/pelican.html","title":"pelican"},{"text":"https://pypi.python.org/pypi/pudb","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/pudb.html","title":"PuDB"},{"text":"decorator class method Django Django test ref: http://mikegrouchy.com/blog/great-django-test-talks.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/python.html","title":"Python"},{"text":"当我们安装某些软件，需要链接python的动态库时，如果系统的python版本是静态库的，会提示如下错误： /usr/bin/ld: .../lib/libpython2.7.a(abstract.o): relocation R_X86_64_32 against 'a local symbol' can not be used when making a shared object; recompile with -fPIC .../lib/libpython2.7.a: could not read symbols: Bad value collect2: ld returned 1 exit status 这种情况需要安装动态库版本。 下面介绍自己编译安装动态库的python版本。 安装 通过ubuntu软件库安装的python版本带有动态链接库，位置为 /usr/lib/libpython<version>.so , 如果系统中默认的python版本太低，软件库中又没有高版本时，需要手动编译安装。 下面以python2.7.4为例，其它版本类似 $ ./configure $ make $ sudo make install 默认情况下， make 会在当前目录下编译生成一个libpython2.7.a的静态链接库。 如果要生成动态链接库，我们需要在 configure 命令后加选项 --enable-shared Building a shared libpython Starting with Python 2.3, the majority of the interpreter can be built into a shared library, which can then be used by the interpreter executable, and by applications embedding Python. To enable this feature, configure with --enable-shared. $ ./configure --enable-shared $ make $ sudo make install 通过这种方式会生成动态链接库，并安装到系统库目录下,地址： /usr/local/lib/libpython2.7.so 问题 在make的过程可能会失败，提示如下错误： /usr/bin/ld: .../lib/libpython2.7.a(abstract.o): relocation R_X86_64_32 against 'a local symbol' can not be used when making a shared object; recompile with -fPIC .../lib/libpython2.7.a: could not read symbols: Bad value collect2: ld returned 1 exit status 原因是系统链接库目前下存在一个表态链接库 libpython2.7.a ，可能是之前安装的， 对于这种情况，我们要把系统原有的Python库的路径从编译参数中除去，让链接器先搜索当前目前，当前路径为\".\"，通过设置LDFLAGS，如下： $ ./configure --enable-shared LDFLAGS = -L. 同时建议：如果你之前运行过 make ，那么在下一次运行 make 之前，运行 make clean 安装完成之后运行 python ，可能会提示如下错误： ImportError: libpython2.7.so.1.0: cannot open shared object file: No such file or directory 这是因为新安装的动态链接库 libpython2.7.so 并不在系统的cache中。 Linux上需要链接动态库时，系统会从cache文件（/etc/ld.so.cache）中找到此链接库。 此时需要运行 sudo ldconfig 更新cache。 参考 http://www.cbug.org/2011/11/21/multiple-python-versions-cause-shared-library-mess.html http://blog.csdn.net/huzhenwei/article/details/7339548 http://linux.101hacks.com/unix/ldconfig/","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/python-build-dynamic-link-library.html","title":"python build Dynamic Link Library"},{"text":"home page: https://github.com/utahta/pythonbrew install packages over pythonbrew pythonbrew can install multiple python versions on the same machine, if you want to install packages over a specified python version, a recommand way is to install pip first. install pip home page: http://www.pip-installer.org/en/latest/ pip is just a python pacakge first use pythonbrew switch to the desired python version, then $ curl - O https : //raw.github.com/pypa/pip/master/contrib/get-pip.py $ [ sudo ] python get - pip . py after that you can use pip to install whatever pacakges over the python version. If you want to install package on a different python version, re-do \"install pip\" section.","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/pythonbrew.html","title":"Pythonbrew"},{"text":"Python Python Guide: http://docs.python-guide.org/en/latest/index.html 字符串匹配的KMP算法: http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/read-later.html","title":"Read Later"},{"text":"关于睡眠你应该知道的十件事 地址：http://v.163.com/movie/2013/5/V/Q/M8TDTH9BJ_M8TE2FFVQ.html 网易分开课上有一期bbc拍摄的关于睡眠的记录片，里面提到了很多关于提高睡眠的问题， 对于一些自己感觉有帮助的记录如下： 咖啡和洒精都会影响人的睡眠；咖啡会让人难以入睡，酒精会容易让人入睡，但会缩深度睡眠阶段。 对于经常跨时区的人来说，睡眠也是个大问题，对于这种情况，可以尝试16小时不吃饭，可以喝水，到达目的地后，在当地的正常用餐时间用餐，按照当地的饮食时间。 经常性的紧张也会影响睡眠，在睡觉前在床上适当做一些放松肌肉的锻炼，先做一些动作让肌肉紧张再放松。 饮食也会影响睡觉，如果多食含蛋白质少的东西会影响发困，多食蛋白质多的会让人清醒。 气味也能促进人的睡眠，睡前喝薰衣草泡的茶及闻薰衣草的气味会有得睡眠。 睡觉前一小时泡个热水澡有利于睡眠。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/sleeping.html","title":"Sleeping"},{"text":"随机MongoDB的数据量增大，我们服务器的磁盘利用率不数据增加，一度接近100%，而且 Disk IO成为MongoDB的性能瓶颈，在进行sharding之前，我们决定先迁移到使用SSD的服务 器，下面我利用 mongoperf 工具对HHD和SSD的 R/W 进行了一个测评。 结果如下： ##Read echo \"{nThreads:32,fileSizeMB:2000,r:true}\" | ./bin/mongoperf ###HHD new thread, total running : 32 2093 ops/sec 8 MB/sec 1963 ops/sec 7 MB/sec 2010 ops/sec 7 MB/sec 2044 ops/sec 7 MB/sec 2061 ops/sec 8 MB/sec 2011 ops/sec 7 MB/sec ###SSD new thread, total running : 32 37804 ops/sec 147 MB/sec 33013 ops/sec 128 MB/sec 36563 ops/sec 142 MB/sec 38056 ops/sec 148 MB/sec 39058 ops/sec 152 MB/sec 39036 ops/sec 152 MB/sec 32946 ops/sec 128 MB/sec 42343 ops/sec 165 MB/sec 40949 ops/sec 159 MB/sec ##Write echo \"{nThreads:32,fileSizeMB:2000,w:true}\" | ./bin/mongoperf ###HHD new thread, total running : 32 3 ops/sec 0 MB/sec 485 ops/sec 1 MB/sec 1543 ops/sec 6 MB/sec 679 ops/sec 2 MB/sec 1059 ops/sec 4 MB/sec 1383 ops/sec 5 MB/sec 674 ops/sec 2 MB/sec 740 ops/sec 2 MB/sec 572 ops/sec 2 MB/sec 679 ops/sec 2 MB/sec 0 ops/sec 0 MB/sec 1134 ops/sec 4 MB/sec ###SSD new thread, total running : 32 read:0 write:1 993 ops/sec 3 MB/sec 2160 ops/sec 8 MB/sec 312 ops/sec 1 MB/sec 493 ops/sec 1 MB/sec 7570 ops/sec 29 MB/sec 10026 ops/sec 39 MB/sec 3120 ops/sec 12 MB/sec 514 ops/sec 2 MB/sec 903 ops/sec 3 MB/sec 1503 ops/sec 5 MB/sec 2413 ops/sec 9 MB/sec 可以看到HHD的读取速度为6-7MB/s，SSD的为150MB/s左右，为HHD的20多倍。 再看写的速度，HHD最高为6MB/s，SSD最高为39MB/s，为HHD的6倍多。 使用SSD后，可以大大减小Disk IO的延时，提高MongoDB响应速度， 但SSD的价格比较高，如果学着SSD性价比低，可以配置sharding。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/ssd-vs-hhd.html","title":"ssd-vs-hhd"},{"text":"gevent: arrow: Better dates & times for Python https://github.com/crsmithdev/arrow http://crsmithdev.com/arrow requests: grequests: undead: https://github.com/waawal/undead legit envoy tablib clint autoenv","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/the-python-packages-i-love.html","title":"The python packages I love"},{"text":"netstat -tanp | grep -o '(10.18.10.20\\|211.151.139.230\\|127.0.0.1):[0-9]*' | sort -nr | uniq -c | sort -nr -k 1 | wc -l","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/time_wait-in-netstat.html","title":"TIME_WAIT in netstat"},{"text":"2013-05-08 TODO TODAY 蕃茄工作法做笔记（思维导图） 总结peclian mysql replication, mysql 索引树","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/todo.html","title":"TODO"},{"text":"当修改某一node的主机名时，修改后此node的所有历史记录都会消失，可以恢复这些历史数据，在munin中所有数据都保存在rrd文件中，此文件在/var/lib/munin中，如：social/services.social-uptime-uptime-g.rrd 是主机名为services.social的其中一个数据文件。当修改了node的主机名时，只需要同时对数据文件重命名即可。 取消munin crontab的执行， cd /etc/cron.d/; sudo mv munin munin.disable crontab不会执行/etc/cron.d/下文件名中带有\".\"的文件 生命名数据文件 修改munin配置/etc/munin/munin.conf中的node主机名 修改munin crontab执行，5分钟后将会看到更新的结果。 NOTE ： 改动数据文件前要先取消crontab的执行，否则可能会影响历史数据, 同时数据文件迁移之后，要保证用户munin有对这些rrd文件写的权限。因为用 sudo cp 之后会改变这些文件的owner. 如果要将munin server转移到其它机器，只需要将rrd数据文件复制过去即可。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/transfer-munin-server.html","title":"transfer munin server"},{"text":"最近要写一个mongo current op的管理工具， http://www.nicosphere.net/urwid-for-python-a-ncurses-library-2541/ https://github.com/intnull/videotop/blob/master/videotop.py Why can't I select text in an Urwid program? By default Urwid's MainLoop tells the terminal that it will handle mouse input so it can react to things like selecting widgets with the mouse or activating check boxes. If you wrote this program and you want to disable Urwid's mouse handling you can set handle_mouse=False when creating your MainLoop or screen object. Or you can just hold the SHIFT key while clicking and dragging in to get the normal select text/copy behavior.","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/urwid.html","title":"urwid"},{"text":"TODO: wrap my plugins auto reload .vimrc augroup reload_vimrc \" { autocmd ! autocmd BufWritePost $ MYVIMRC source $ MYVIMRC augroup END \" } http://www.bestofvim.com/tip/auto-reload-your-vimrc/","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/vim.html","title":"vim"},{"text":"最近在分析mongo的性能问题，发现mongo所在机器的io比较频繁。 使用vmstat命令查看，发现b（The number of processes in uninterruptible sleep）这一列的数量比较大，持续在4左右。 随后查询uninterruptible sleep状态的意义，引自：http://www.novell.com/support/kb/doc.php?id=7002725 Processes in a \"D\" or uninterruptible sleep state are usually waiting on I/O. The ps command shows a \"D\" on processes in an uninterruptible sleep state. The vmstat command also shows the current processes that are \"blocked\" or waiting on I/O. The vmstat and ps will not agree on the number of processes in a \"D\" state, so don't be too concerned. You cannot kill \"D\" state processes, even with SIGKILL or kill -9. As the name implies, they are uninterruptible. You can only clear them by rebooting the server or waiting for the I/O to respond. It is normal to see processes in a \"D\" state when the server performs I/O intensive operations. If performance becomes an issue, you may need to check the health of your disks. Make sure your firmware and kernel disk drivers are updated. In the example above, there is heavy disk activity shown in the \"io\" columns and the server is currently swapping to disk. The example more likely represents a memory issue, rather than a disk I/O issue. There are two ways to find more about the processes in D state. ps -eo ppid,pid,user,stat,pcpu,comm,wchan:32 This prints a list of all processes where in the last column either a '-' is displayed when the process is running or the name of the kernel function in which the process is sleeping if the process is currently sleeping. This includes also processes which are interruptible. Processes that are in uninterruptible sleep can be determined via the fourth column which would then show a D. echo w > /proc/sysrq-trigger This command produces a report and a list of all processes in D state and a full kernel stack trace to /var/log/messages. This shows much more information than the first option described above. 关于vmstat的使用参考：http://www.linuxintheshell.org/2013/05/22/episode-030-vmstat/","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/vmstat.html","title":"vmstat"},{"text":"http://lucumr.pocoo.org/2007/5/21/getting-started-with-wsgi/ http://webpython.codepoint.net/wsgi_request_parsing_post http://blog.pythonisito.com/2012/08/building-web-applications-with-gevents.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/wsgi.html","title":"wsgi"},{"text":"alias 对于一些比较常用的长命令可以用alias做一个别名，以后可以直接用别名操作。 e.g. 在.bashrc中加入 alias servername = 'ssh sns@hostname' 执行： $ servername 即可登陆到hostname机器。 rsync 大家常用rsync同步数据，有些数据是不需要同步的，可以用 --exclude 选项。 e.g. rsync -avz --exclude \".*\" --exclude \"dbconfig.py\" src user@hostname:/dest --exclude \".*\" 不会同步以 \".\" 开头的文件或文件夹，不会同步.git目录（.git目录是比较大的，不建议同步）。 mail 所有crontab任务的运行结果都会输出到mail中，可以随时查看每个任务的运行情况。 tmux 工作中经常ssh到远程服务器，有时需要在远程服务器中开启多个终端， 一种办法是在本地开启多个终端，分别远程到服务器。 还有一种方式是利用 tmux 。 install: $sudo apt-get install tmux basic usage: < C - b p > go to previous window < C - b n > go to next window < C - b c > create a new window < C - b 1 > go to No .1 window < C - b d > detach this tmux session tmux 的功能非常强，但命令较多，学习成本较高，可以先使用上面几个命令。 推荐介绍视频: http://happycasts.net/episodes/41?autoplay=true ssh 今天搞清楚了ssh的标准输入输出，下面是ssh命令的使用方式 ssh [ - 1246 AaCfgKkMNnqsTtVvXxYy ] [ - b bind_address ] [ - c cipher_spec ] [ - D [ bind_address : ] port ] [ - e escape_char ] [ - F configfile ] [ - I pkcs11 ] [ - i identity_file ] [ - L [ bind_address : ] port : host : hostport ] [ - l login_name ] [ - m mac_spec ] [ - O ctl_cmd ] [ - o option ] [ - p port ] [ - R [ bind_address : ] port : host : hostport ] [ - S ctl_path ] [ - W host : port ] [ - w local_tun [ : remote_tun ]] [ user @ ] hostname [ command ] 最后的 command 是在 hostname 机器上执行 command 命令，那么 command 命令的标准输入输出是远程机器 hostname 上，还是在本地机器上？ 很多初学者可能会认为是远程机器的标准输入输出（本人以前也是这么认为的），其实是在本地机器上。 下面我们做个试验，可以在本地执行远程机器的程序并获得输出结果。 $echo \"local host\" | ssh user@hostname \"(echo server; cat; echo server)\" server local host server 我们用将\"local host\"输出到标准输出，在hostname机器上 cat 从标准输入获取\"local host\"并再次输出到标准输出(本地机器的标准输出)。 虽然 command 中的标准输入输出是在本地机器，但 command 中的命令和目录等都是相对于hostname的，与本地机器无法。 fuser","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/bash.html","title":"Bash"},{"text":"Basic 使用Git前，要先理解三个概念（与其它的VCS不同）：working directory, index, repository. working directory 即是你当前的工作目录，你所编辑的文件都在此目录下。 index git add Git Flow http://danielkummer.github.com/git-flow-cheatsheet/ Git submodules http://git-scm.com/docs/gitignore http://git-scm.com/docs/git-check-ignore.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/git.html","title":"Git"},{"text":"Linux TCP Performance Tuning There are two ways to change tcp parameters. change the value of parameter files in /proc/sys/net/ e.g. increase the value of somaxconn echo 1024 > /proc/sys/net/core/somaxconn when the server restart, the parameter will restore. change the vlaue in /etc/sysctl.con, and run sudo sysctl -p to apply the changes immediately, this can change the value permanently. Parameters /proc/sys/fs/file-max: The maximum number of concurrently open files. /proc/sys/net/ipv4/tcp_max_syn_backlog: Maximum number of remembered connection requests, which are still did not receive an acknowledgment from connecting client. The default value is 1024 for systems with more than 128Mb of memory, and 128 for low memory machines. If server suffers of overload, try to increase this number. /proc/sys/net/core/somaxconn: Limit of socket listen() backlog, known in userspace as SOMAXCONN. Defaults to 128. The value should be raised substantially to support bursts of request. For example, to support a burst of 1024 requests, set somaxconn to 1024. change userlevel file-max need logout and login to work","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/linux.html","title":"Linux"},{"text":"emacs with zsh when I first open zsh in emacs using: M - x term It always print strange characters like \"4m\", that's because I don't have eterm-color terminfo, I solve this problem by running: # If you use Cocoa Emacs or Carbon Emacs tic - o ~/ . terminfo / Applications / Emacs . app / Contents / Resources / etc / e / eterm - color . ti in terminal. ref: http://stackoverflow.com/questions/8918910/weird-character-zsh-in-emacs-terminal","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/problems-i-encouter-with-emacs.html","title":"Problems I encouter with emacs"},{"text":"Redis and MongoDB insertion performance analysis: http://blog.axant.it/archives/236 MongoDB and Redis: a different interpretation of what's wrong with Relational DBs: http://oldblog.antirez.com/post/MongoDB-and-Redis.html","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/04/09/redis-vs-mongodb.html","title":"Redis vs Mongodb"},{"text":"When I write something, I'd like to use graphes, graph is a great way to explain ideas, it can save a lot of words, and express more clearlly. Is there a good tool which make you drawing graphes like programming, yes there is, the great Graphviz . The first time I saw it, I was shocked by its powerful, it can draw some many kinds of beautiful graphes. You can learn graphviz through its offcial website , the documents there are a little hard for newbies, and as a know there is no much toturials for graphviz when I'm writting this, so I want to write a tutorial for it. More people need to know this tool. What is Graphviz From offcial website: Graphviz is open source graph visualization software. Graph visualization is a way of representing structural information as diagrams of abstract graphs and networks. It has important applications in networking, bioinformatics, software engineering, database and web design, machine learning, and in visual interfaces for other technical domains. Graphviz package shiped with some programs and libs, the programs can take descriptions of graphs in a text language( The DOT Language )), and generate graphs in various useful formats, like png, svg, pdf, ps. Graphviz has many userful features, you can custome colors, fonts, styles. I will introduce to you two command line programs, dot and neato . They are enough for general use, if you have special needs, look into the documents . dot, neato dot : a utility program for drawing directed graphs. neato : a utility program for drawing undirected graphs. They have the same usages. Run dot -? for help. dot - Tsvg hello_world . dot - o hello_world . svg dot take hello_world.dot as input, and generate svg file hello_world.svg . -T option takes graph format, you can assign other formats, like png, pdf, ps. hello_world.dot: digraph hello { n1 [label=\"Hello\"] n2 [label=\"World!\"] n1 -> n2 } hello_world.svg: hello_world.dot is a description in dot language. The DOT Language Now I introduce the dot language to you. Grammar defination: Terminals are shown in bold font and nonterminals in italics. Literal characters are given in single quotes. Parentheses ( and ) indicate grouping when needed. Square brackets [ and ] enclose optional items. Vertical bars | separate alternatives. graph : [ strict ] ( graph | digraph ) [ ID ] '{' stmt_list '}' stmt_list : [ stmt [ ';' ] [ stmt_list ] ] stmt : node_stmt | edge_stmt | attr_stmt | ID '=' ID | subgraph attr_stmt : ( graph | node | edge ) attr_list attr_list : '[' [ a_list ] ']' [ attr_list ] a_list : ID '=' ID [ ( ';' | ',' ) ] [ a_list ] edge_stmt : ( node_id | subgraph ) edgeRHS [ attr_list ] edgeRHS : edgeop ( node_id | subgraph ) [ edgeRHS ] node_stmt : node_id [ attr_list ] node_id : ID [ port ] port : ':' ID [ ':' compass_pt ] | ':' compass_pt subgraph : [ subgraph [ ID ] ] '{' stmt_list '}' compass_pt : ( n | ne | e | se | s | sw | w | nw | c | _ ) An ID is one of the following: Any string of alphabetic ( [a-zA-Z\\200-\\377] ) characters, underscores ( '_' ) or digits ( [0-9] ), not beginning with a digit; a numeral [ - ] ? ( . [ 0 - 9 ] + | [ 0 - 9 ] + ( . [ 0 - 9 ] * ) ? ); any double-quoted string (\"...\") possibly containing escaped quotes ('\") 1 ; an HTML string (<...>). It's pretty abstract and not clear for newbies. I'll explain to you by examples, once you understand these grammars, you can draw graphes freely. We start with hello_world.dot , /* * graph: [ strict ] (graph | digraph) [ ID ] '{' stmt_list '}' * * hello is ID, the body is the \"stmt_list\" */ digraph hello { /* node_stmt: node_id [ attr_list ] * node_id: ID [ port ] * attr_list :'[' [ a_list ] ']' [ attr_list ] * a_list: ID '=' ID [ (';' | ',') ] [ a_list ] * * n1 is ID of a node_id * '[label=\"Hello\"]' is attr_list * 'label=\"Hello\"' is a_list */ n1 [ label = \"Hello\" ] n2 [ label = \"World!\" ] /* edge_stmt: (node_id | subgraph) edgeRHS [ attr_list ] * edgeRHS: edgeop (node_id | subgraph) [ edgeRHS ] * * 'n1' is node_id * '-> n2' is edgeRHS * '->' is edgeop * 'n2' is node_id */ n1 -> n2 } An edgeop is \"->\" in directed graphs and \"--\" in undirected graphs. Next we add attr to graph attr_stmt : ( graph | node | edge ) attr_list attr: http://graphviz.org/content/attrs node-shapes: http://graphviz.org/content/node-shapes arrow-shapes: http://graphviz.org/content/arrow-shapes colors: http://graphviz.org/doc/info/colors.html Ref http://graphviz.org/ http://www.linuxjournal.com/article/7275?page=0,0","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/23/graphviz-tutorial.html","title":"Graphviz Tutorial"},{"text":"As we all know MongoDB use B-tree to create indexes, here I'll show the deep view of MongoDB indexes. B-tree First, an overview of B-tree. From wikipedia: In computer science, a B-tree is a tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a binary search tree in that a node can have more than two children (Comer 1979, p. 123). Unlike self-balancing binary search trees, the B-tree is optimized for systems that read and write large blocks of data. It is commonly used in databases and filesystems. A B-tree of order 2 or order 5: Internal nodes can have vary number of keys, vary between $d$ and $2d$, the factor of $2$ can guarantee that nodes can be split and combined, and still conform to the upper and lower limit. All leaf nodes have the same depth. Definition from wikipedia: According to Knuth's definition, a B-tree of order m is a tree which satisfies the following properties: Every node has at most m children. Every non-leaf node (except root) has at least ⌈m⁄2⌉ children. The root has at least two children if it is not a leaf node. A non-leaf node with k children contains k−1 keys. All leaves appear in the same level, and internal vertices carry no information. MongoDB Index B-tree In this section, I will show you mongo index btree. From btree.h : The nodes of our btree are referred to as buckets below. These buckets are of size BucketSize and their body is an ordered array of pairs, where disk loc is the disk location of a document and bson key is a projection of this document into the schema of the index for this btree. Ordering is determined on the basis of bson key first and then disk loc in case of a tie. All bson keys for a btree have identical schemas with empty string field names and may not have an objsize() exceeding KeyMax. The btree's buckets are themselves organized into an ordered tree. This is what btree looks like: When the btree is serialized on the disk, every bucket is stored as a record, like a document in a collection. Each bucket has a fixed size 8192, but with 16 byte to store record header. See mongo storage . Bucket store keynode and keydata, this is what bucket looks like: prevChildBucket is a pointer, point to the left child bucket of this key. kdo points to the key data of this key. recordLoc points to the location of the key's doucment. keynode is a struct, and has a fixed size. template < class Loc > struct __KeyNode { /** * The 'left' child bucket of this key. If this is the i-th key, it * points to the i index child bucket. */ Loc prevChildBucket ; /** The location of the record associated with this key. */ Loc recordLoc ; /** Offset within current bucket of the variable width bson key for this _KeyNode. */ unsigned short _kdo ; } The size of keydata varies, with upper limit 1024 bytes. When insert a new key to a bucket, keynode is inserted from left, and keydata is insert from right. Bucket format: | hhhh | kkkkkkk ———— bbbbbbbbbbbuuubbbuubbb | h = header data k = KeyNode data - = empty space b = bson key data u = unused ( old ) bson key data , that may be garbage collected So how many keys can be stored in a bucket depends on keydata size. MongoDB allows to store 1024 keys at most. When the bucket is full, or has 1024 keys, it will be splited. keydata is a projection of a document into the schema of the index. For an index, the schema is fixed, so keydata does not need to contain fieldNames. If a document does not have a field, then the fileValue will be null in keydata. Sparse: When create a sparse index, only when the document does not have all the fields of the index, it will be ignore, if one of the fields exists, it will be indexed. Example: index: { name : 1 , age : 1 } documents: d1: { name : ' Tom ' , age : 23 } keydata : Tom , 23 d2: { name : null , age : 40 } keydata : null , 40 d3: { name : ' Jerry ' , address : CA } keydata : Jerry , null d4: { weight : 70 , address : CA } keydata : null , null If create the index with sparse as true, the document d4 will not be indexed. IndexStats MongoDB 2.4 ships with a indexStats command, the command can be run only on a mongod instance that uses the --enableExperimentalIndexStatsCmd option. To aggregate statistics, issue the command like so: db.runCommand ( { indexStats : \"<collection>\" , index : \"<index name>\" } ) Reference http://en.wikipedia.org/wiki/B-tree https://github.com/mongodb/mongo/blob/master/src/mongo/db/structure/btree/btree.h https://github.com/mongodb/mongo/blob/master/src/mongo/db/structure/btree/btree.cpp","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/19/mongodb-index-internals.html","title":"MongoDB Index Internals"},{"text":"本文我们讨论一下一些特殊查询对索引的使用情况。 Test data: rs0 : PRIMARY > db . foo . find () { \"_id\" : ObjectId ( \"52fda635720bcc4ea4bb961c\" ), \"name\" : \"test_name\" } { \"_id\" : ObjectId ( \"5304412b720bcc4ea4bb9627\" ), \"b\" : 1 } { \"_id\" : ObjectId ( \"5304430b720bcc4ea4bb9628\" ), \"name\" : null } rs0 : PRIMARY > db . foo . ensureIndex ({ 'name' : 1 }) rs0 : PRIMARY > db . foo . ensureIndex ({ 'b' : 1 }) $exists, null // not use index {name: 1} rs0: PRIMARY > db . foo . find ({' name ' : { $ exists: true }}) { \"_id\" : ObjectId ( \"52fda635720bcc4ea4bb961c\" ), \"name\" : \"test_name\" } { \"_id\" : ObjectId ( \"5304430b720bcc4ea4bb9628\" ), \"name\" : null } {$exists: true} will not use index {name: 1} . // use index {name: 1} rs0: PRIMARY > db . foo . find ({' name ' : { $ exists: false }}, {' name ' : 1 }) { \"_id\" : ObjectId ( \"5304412b720bcc4ea4bb9627\" ), \"b\" : 1 } {$exists: false} can use the index {name: 1} . // use index {name: 1} rs0: PRIMARY > db . foo . find ({' name ' : null }, {' name ' : 1 }) { \"_id\" : ObjectId ( \"5304412b720bcc4ea4bb9627\" ), \"b\" : 1 } { \"_id\" : ObjectId ( \"5304430b720bcc4ea4bb9628\" ), \"name\" : null } The doucment without name filed also show up. So when create an index on a field without sparse option, if an document without the filed, it will also be indexed in the index with the filed value as null . Use explain() on the two query, the indexBounds of the output are the same. They both use index {name: 1} , and have the same time complexity, only the outputs are different. \"nscannedObjects\" : 2 , \"nscanned\" : 2 , \"indexBounds\" : { \"name\" : [ [ null , null ] ] } , $ne // use index {b: 1} rs0: PRIMARY > db . foo . find ({ 'b' : { $ ne: 1 }}) { \"_id\" : ObjectId ( \"52fda635720bcc4ea4bb961c\" ), \"name\" : \"test_name\" } { \"_id\" : ObjectId ( \"5304430b720bcc4ea4bb9628\" ), \"name\" : null } explain(): \"indexBounds\" : { \"b\" : [ [ { \"$minElement\" : 1 }, 1 ] , [ 1 , { \"$maxElement\" : 1 } ] ] } NOTE: _id is output by default for any query, so if you want an query to be indexOnly, you need to specify _id: 0 on return fields.","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/19/mongodb-index.html","title":"MongoDB Index"},{"text":"启用journal后，MongoDB的操作先写到journal buffer中，每100ms MongoDB会flush journal buffer到磁盘journal中，此时数据不会丢失， 若MongoDB crash，则最多会丢失100ms的数据。 Journal Files mongod启动后，会在 dbpath 下新建一个 journal 目录，这个目录下存放所有的journal文件，journal 文件是append-only，而且文件名以 j._ 为前缀，每个journal文件的大小为1GB，当journal超过1GB后，会创建新的journal文件，当MongoDB将journal中的所写操作应用到数据文件后，这些journal文件才会被删除。 journal文件的格式为： How Journal Works NOTE: 此部分翻译自： http://www.kchodorow.com/blog/2012/10/04/how-mongodbs-journaling-works/ 首先磁盘上存储了 journal files和data files，如下： 当启动mongod后，会将data files映射到shared view，并返回其在内存的虚拟地址。例如：数据文件大小为2,000 bytes，操作系统将其映射到内存地址1,000,000-1,002,000，如果你访问地址1,000,042，那你会得到数据文件中从第42个字节开始的数据。（当然操作系统不会把所有数据都load到内存中，只有当你访问到时，才会放到内存中.） shared view由文件直接映射，当修改了内存中的数据后，OS会将修改flush到对应的数据文件中。这是在没有journal的情况下mongod的工作方式，mongod会每60s将内存中的修改flush到磁盘。 如果启用了journal，mongod会再做一次映射，将shared view映射到private view，这就是为什么启用journal后虚拟内存的使用量翻倍。 由于private view是由shared view映射来，所以private view的改动不能直接flush到硬盘。 当mongod进行写操作时，会修改private view。 然后mongod会将这些改动的描述写到journal file, 在journal file中记录哪些文件的哪些字节被修改。 改动的描述会被追加到journal file之后。 这时写操作被持久化，不会丢失。如果mongod crash，虽然改动没记录到data file中，journal还可以replay这部分改动。 接下来journal会replay shared view中的改动。 然后mongod再重新将shared view映射到private view中，防止private view中有太多的改动。 最后，shared view中的改动flush后硬盘中。默认情况下mongod会要求OS每60s flush一次。每次flush完后，会将journal中flush过的改动删除。 这就是journal的工作原理。感谢Richard，他给的解释是我听过的最好的（今年秋季他还会 在线教授 MongoDB ）。 Cost of a Journal 使用journal后写操作的性能会降低5-30%。 对于写操作非常频繁的系统建议journal files和data files使用不同的物理磁盘，它们都会flush数据到磁盘，若使用同一个会使用Disk IO延时增大，降低性能。 对于以读为主的系统，不会有太多影响。 参考 http://www.slideshare.net/mongodb/mongodb-london-2013understanding-mongodb-storage-for-performance-and-data-safety-by-christian-kvalheim-10gen http://www.kchodorow.com/blog/2012/10/04/how-mongodbs-journaling-works/","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/18/mongodb-journal.html","title":"MongoDB Journal"},{"text":"MongoDB 使用系统调用 mmap 将数据文件映射到内存，然后直接操作内存。 这样简化了MongoDB的开发，可以省去复杂的内存及磁盘操作相关的代码，完全不用关心文件系统的类型，OS会自动cache数据，并使用LRU的方式，而且MongoDB重启后可以继续使用cache中的数据。 当然也会有缺点，数据文件碎片(fragmentation)会影响内存的使用，而且操作系统的 read-ahead 也会影响内存的使用，对于索引数据来说，cache的LRU方式并不合适，索引数据最好一直在内存中。 MongoDB 进程的虚拟内存 virtual size = total files size + overhead(connections, heap) 如果MongoDB启动时启用了journal，则virtual size会翻倍。 TODO: add mmap fundermentals 参考 http://www.slideshare.net/mongodb/mongodb-london-2013understanding-mongodb-storage-for-performance-and-data-safety-by-christian-kvalheim-10gen","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/18/mongodb-mmap.html","title":"MongoDB mmap"},{"text":"我们都知道MongoDB通过 mmap 的方式将存储在磁盘上的数据文件映射到内存中进行操作，那MongoDB是如何组织数据文件的，最近在网上找了相关资源，在此做一个 翻译和汇总 , 非原创，原文在下面的参考链接部分。 我们按照从总体到内部的顺序进行分析， Data Files 所有的 data files 存储在 dbpath 参考所指定的目录中，对应于每个数据库都有一个 、namespace file, 多个journal file 和 data file。 $ ls - lh / data / db drwxr - xr - x 2 mongodb nogroup 4.0 K Feb 14 13 : 14 journal - rwxr - xr - x 1 mongodb nogroup 6 Feb 13 16 : 49 mongod . lock - rw ------- 1 mongodb nogroup 64 M Feb 14 13 : 15 test .0 - rw ------- 1 mongodb nogroup 128 M Feb 14 13 : 14 test .1 - rw ------- 1 mongodb nogroup 16 M Feb 14 13 : 15 test . ns drwxr - xr - x 2 mongodb nogroup 4.0 K Feb 14 13 : 15 _tmp mongod.lock 是MongoDB的lock 文件，可用来判断上次MongoDB是否正常shutdown。 其余的所有文件为 test 数据库的文件。 MongoDB 采用 aggressive pre-allocation的方式申请 data files，而且总会多申请一个备用的data file，如上面的test.1为备用的data file。数据文件会以指数级增长，最大为2GB。 namespace file 内存储了所有的 collection以及index。 data file 存储了所有的document及index。data file 以 extent为逻辑存储单元，每个data file包含多个extents. Extents 每个extent被用来存储doucments或者index。 extents与data file之间的关系 extents与namespace file之间的关系 NOTE : 一个extent只能存储一个collection的documents或index，不能同时用来存储documents和index。 一个collection通常会有多个extents。 当需要一个新的extent时会在当前data file中申请extent，如果当前data file空间不足，则申请新的data file. 同一个collection的所有extents通过指针连接，namespace file中的collection只需要指向其第一个extent即可。 每个extent会存储一些metadata，其余空间存储records。 Records 每一个record会存储一些metadata及一个document。 Indexes MongoDB的索引是BTree结构，序列化到磁盘进行存储，存储在自己的extents中，而且每一个index有一个单独的namespace，并不属于其collection的namespace. > db . system . namespaces . find () { \"name\" : \"test.system.indexes\" } { \"name\" : \"test.foo.$_id_\" } { \"name\" : \"test.foo\" } Metrics from db.stats() 现在我们知道了MongoDB的数据组织方式，下面分析一下 db.stats() 的输出指标所代表的含义。 dataSize dataSize 是所有 documents 的大小总和，包括这个document的padding，也是所有records的总和, 当document被删除时dataSize会变小，但当减小document大小时，dataSize不会变化。 storageSize storageSize是所有extents的大小总和，会比dataSize要大，因为它会包括extents中未被使用的空间，以及因document被删除及移动带来的空闲空间。 当删除或减小document时，storageSize不会变化。 fileSize fileSize 包括所有的data extents, index extents以及data file中未使用的空间，是数据库存储在磁盘上的文件大小。会比storageSize要大，因为它还包括index extents，以及未使用的空间。 NOTE: dataSize 和 storageSize 都不包括index。 当删除数据库时，fileSize会变化，因为此数据库相应的data file会被删除，但当删除collection，documents及index时fileSize不会变化。 nsSize namespace( test.ns )文件的大小。namespace file的大小是固定的，但可以通过修改nssize参数调整。 Fragmentation 当执行 update 及 remove 操作时会产生fragmentation. 如果文档的大小不固定，而且经常发生变化，则会产生大量的 fragmentation，这样会浪费内存及磁盘空间，增加Disk IO，而且由于update操作引起的文档移动还会导致索引的更新，使写操作变慢。可以通过比较 db.collection.stats() 输出中的 size 和 storageSize 来判断fragmentation的状况。 How to Combat 定时执行 compact ，会lock数据库，要在secondary上执行。 设定 collection schema, 使document不会增大 pre-pad documents, 使document不会增大。 使用不同的collection, 尽量使用 db.collection.drop() 代替 db.collection.remove() 删除数据 设置 usePowerOf2sizes 提高空间的重用度。 MongoDB Exorcises File Fragmentation Demons 介绍了 usePowerOf2sizes 所带来的空间利用率及性能的提升。不同的应用场景其效果也不一样。 参考 http://www.slideshare.net/mongodb/mongodb-london-2013understanding-mongodb-storage-for-performance-and-data-safety-by-christian-kvalheim-10gen http://blog.mongolab.com/2014/01/how-big-is-your-mongodb/ http://linux.sys-con.com/node/2756958/mobile","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/18/mongodb-storage-internals.html","title":"MongoDB Storage Internals"},{"text":"今天在使用 fabric 中的 put 函数上传文件到服务器时，抛出 Connection closed 的异常，Google 之后发现 put 使用 sftp 进行上传，于是使用 sftp 连接服务器： $ sftp user @ domain . org Connecting to domain . org ... user @ domain . org ' s password : subsystem request failed on channel 0 Connection closed 发现连接失败. 解决方案： https://forums.gentoo.org/viewtopic-t-802682-start-0.html In your /etc/ssh/sshd_config (not ssh_config) file, you probably have a line like this: Code: Subsystem sftp / usr / lib / misc / sftp - server If so, it's the cause of this error message. That's especially true if your sftp user is logging into a chrooted environment, where \"/usr/lib\" probably does not exist. My own sftp server is configured this way. However, SSHD has the sftp functionality built-in and does not need to execute an external \"helper\" program like that. So, if you have a line like the above, it can be fixed by changing it to: Code: Subsystem sftp internal - sftp","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/17/sftp-in-ssh-config.html","title":"sftp in ssh config"},{"text":"ack 是一个类似于grep的工具，专门针对程序员进行了优化，主要用于搜索源代码，默认忽略非代码文件，支持大部分编程语言。在各平台的安装参考其官方网站 http://beyondgrep.com/ 。 ack.vim 是ack的vim插件，可在vim下直接使用ack。 ack可通过配置文件调整搜索行为，如增加新编程语言的技术等。 类似于大部分unix程序，ack有全局配置(/etc/ackrc)，用户配置(~/.ackrc)，工程配置(.ackrc)等，可参考其manpage。 下面列出本人的用户配置(~/.ackrc): -- smart - case -- sort - files -- type - set = rst : ext : rst , txt -- type - set = md : ext : mkd , md , markdown -- type - set = dotfile : match :/&#94; \\ .. +/ -- nodotfile 增加对Markdown和reStructuredText文件的支持，并忽略所有以\".\"开头的隐藏文件。","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/04/vim-ack.html","title":"ack.vim"},{"text":"You will be newbie forever. Get good at the beginner mode, learning new programs, asking dumb questions, making stupid mistakes, soliciting help, and helping others with what you learn (the best way to learn yourself). From: http://benoithamelin.tumblr.com/post/17949523929/you-will-be-newbie-forever-get-good-at-the","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/02/03/be-newbie-forever.html","title":"Be Newbie Forever"},{"text":"http://virtualthreads.blogspot.com/2006/02/understanding-memory-usage-on-linux.html http://www.cyberciti.biz/tips/howto-find-memory-used-by-program.html pmap vmmap on mac","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/01/01/mem.html","title":"mem"},{"text":"/etc/sysconfig/iptables - The system scripts that activate the firewall by reading this file. sudo vi / etc / iptables . up . rule sudo iptables - restore < / etc / iptables . up . rule Ref: http://www.cyberciti.biz/faq/rhel-fedorta-linux-iptables-firewall-configuration-tutorial/","tags":"misc","loc":"http://zhangliyong.github.io/posts/2014/01/01/iptables.html","title":"iptables"},{"text":"当在终端下执行某些操作，提示编码问题时，基本是因为终端的locale环境的编码不支持。 有些终端下默认的LC_*设置为 C ，可运行 locale 查看，也可查看 /etc/default/locale ， 此时终端环境的默认编码是 ASCII 。 可将其改为 UTF8 编码。 修改方式如下： 用 locale -a 命令查看系统支持的category。 在终端下进行 export LANG=<category> ，此时终端的 LC_* 变量发生了变化。 如果不想每次手动设置，可将 export LANG=<category> 加到 .bashrc 中。 如： echo \"export LANG=en_US.UTF-8\" >> . bashrc","tags":"misc","loc":"http://zhangliyong.github.io/posts/2013/12/22/locale.html","title":"locale"}]}